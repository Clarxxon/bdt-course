# Список практических работ

1. Развернуть ПО для распределенных вычислений Spark и проверить работоспособность на тестовых функциях.
2. Развернуть ПО на основе HDFS в связке с кластером Apache Spark для выполнения вычислений и взаимодействия с файлами; проверить его работоспособность на тестовых функциях.
3. Развернуть и использовать систему потоковой обработки данных Apache Kafka, включающую Zookeeper для координации работы брокеров, сам брокер Kafka для приёма и хранения сообщений, а также веб-интерфейс Kafdrop для мониторинга топиков и просмотра поступающих данных. Для проверки работы системы запустить отправитель (продюсер), генерирующий тестовые сообщения и отправляющий их в топик, и получатель (консьюмер), позволяющий убедиться в корректности передачи данных.
4. Развернуть и настроить ПО на основе Elasticsearch и Kibana в среде Docker. Изучить принцип работы конфигурации Nginx и проверить работу кластера.
5. Развернуть и настроить локальный стек ELK с использованием Docker для сбора, хранения и визуализации логов: создать кластер, включающий Elasticsearch для хранения данных, Kibana для их анализа и визуализации, Filebeat для автоматического сбора логов из файлов и передачи их в Elasticsearch, а также тестовый Nginx-сервер, генерирующий реальные логи для проверки работы системы. Написать 3-5 аналитических запроса для демонстрации работы с фильтрацией сообщений.
6. Развернуть и настроить локальный стек для интерактивной визуализации данных на базе Apache Superset. После успешного подключения загрузить и визуализировать данные из датасета для дальнейшего построения дашбордов.
7. Развернуть и настроить локальный стек для интерактивной визуализации данных на базе Prometheus и Grafana. Подключить и запустить exporter (например, node_exporter) на хостовой машине или в контейнере. В Grafana создать несколько простых дэшбордов. Сделать снимки дашбордов, описать их поведение и пояснение по метрикам и интерпретации.
