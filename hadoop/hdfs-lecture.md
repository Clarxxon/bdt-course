# –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ HDFS –∏ Apache Spark (PySpark)


1. –ö–æ—Ä–æ—Ç–∫–æ: —á—Ç–æ —Ç–∞–∫–æ–µ HDFS –∏ Spark –∏ –ø–æ—á–µ–º—É –∏—Ö –≤–º–µ—Å—Ç–µ –∏—Å–ø–æ–ª—å–∑—É—é—Ç
	-	HDFS (Hadoop Distributed File System) ‚Äî —Ä–∞—Å–ø—Ä–µ–¥–µ–ª—ë–Ω–Ω–∞—è —Ñ–∞–π–ª–æ–≤–∞—è —Å–∏—Å—Ç–µ–º–∞ –¥–ª—è –±–æ–ª—å—à–∏—Ö –æ–±—ä—ë–º–æ–≤ –¥–∞–Ω–Ω—ã—Ö: —Ö—Ä–∞–Ω–∏—Ç —Ñ–∞–π–ª—ã, —Ä–∞–∑–±–∏–≤–∞–µ—Ç –∏—Ö –Ω–∞ –±–ª–æ–∫–∏ –∏ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª—è–µ—Ç –ø–æ DataNode. –ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ —Ö—Ä–∞–Ω–∏—Ç NameNode.
–î–∞—ë—Ç –Ω–∞–¥—ë–∂–Ω–æ–µ –∏ –º–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º–æ–µ —Ö—Ä–∞–Ω–∏–ª–∏—â–µ –¥–ª—è –∞–Ω–∞–ª–∏—Ç–∏–∫–∏.
(–¥–ª—è –¥–µ—Ç–∞–ª–µ–π: WebHDFS / –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è —Å–º. –æ—Ñ–∏—Ü–∏–∞–ª—å–Ω—É—é –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—é).
	-	Apache Spark ‚Äî –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω–∞—è –ø–ª–∞—Ç—Ñ–æ—Ä–º–∞ –¥–ª—è —Ä–∞—Å–ø—Ä–µ–¥–µ–ª—ë–Ω–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏ –¥–∞–Ω–Ω—ã—Ö (–ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã–µ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏–∏, SQL, —Å—Ç—Ä–∏–º–∏–Ω–≥, ML).
Spark –º–æ–∂–µ—Ç —á–∏—Ç–∞—Ç—å/–ø–∏—Å–∞—Ç—å –¥–∞–Ω–Ω—ã–µ –ø—Ä—è–º–æ –∏–∑ HDFS –∏ –≤—ã–ø–æ–ª–Ω—è—Ç—å –≤—ã—á–∏—Å–ª–µ–Ω–∏—è –Ω–∞–¥ –Ω–∏–º–∏.
–ü—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–µ—Å–∫–æ–ª—å–∫–æ API:
	-	RDD (–Ω–∏–∑–∫–æ—É—Ä–æ–≤–Ω–µ–≤–æ–µ)
	-	DataFrame / SQL (—Ç–∞–±–ª–∏—á–Ω–æ–µ, –æ–ø—Ç–∏–º–∏–∑–∏—Ä—É–µ–º–æ–µ)
	-	Dataset (–≤ Scala/Java)

üìå –ü–æ—á–µ–º—É –≤–º–µ—Å—Ç–µ?
HDFS ‚Äî –º–µ—Å—Ç–æ —Ö—Ä–∞–Ω–µ–Ω–∏—è, Spark ‚Äî –¥–≤–∏–∂–æ–∫ –æ–±—Ä–∞–±–æ—Ç–∫–∏.
HDFS –≥–∞—Ä–∞–Ω—Ç–∏—Ä—É–µ—Ç —Ä–µ–ø–ª–∏–∫–∞—Ü–∏—é –∏ –æ—Ç–∫–∞–∑–æ—É—Å—Ç–æ–π—á–∏–≤–æ—Å—Ç—å, Spark ‚Äî –±—ã—Å—Ç—Ä–æ–µ in-memory/–¥–∏—Å–∫–æ–≤–æ–µ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–µ.

---

2. –ö—Ä–∞—Ç–∫–∞—è —Ç–µ—Ä–º–∏–Ω–æ–ª–æ–≥–∏—è
	-	RDD (Resilient Distributed Dataset) ‚Äî —Ñ—É–Ω–¥–∞–º–µ–Ω—Ç Spark: –∏–º–º—É—Ç–∞–±–µ–ª—å–Ω–∞—è —Ä–∞—Å–ø—Ä–µ–¥–µ–ª—ë–Ω–Ω–∞—è –∫–æ–ª–ª–µ–∫—Ü–∏—è —ç–ª–µ–º–µ–Ω—Ç–æ–≤.
	-	–õ–µ–Ω–∏–≤—ã–µ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏–∏ –∏ –¥–µ–π—Å—Ç–≤–∏—è.
	-	–î–∞—ë—Ç –ø–æ–ª–Ω—ã–π –∫–æ–Ω—Ç—Ä–æ–ª—å, –Ω–æ –±–µ–∑ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–π.
	-	DataFrame (–¢–∞–±–ª–∏—á–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ) ‚Äî —Å—Ç—Ä—É–∫—Ç—É—Ä–∞ —Å –∏–º–µ–Ω–æ–≤–∞–Ω–Ω—ã–º–∏ –∫–æ–ª–æ–Ω–∫–∞–º–∏, –∫–∞–∫ —Ç–∞–±–ª–∏—Ü–∞ –≤ –ë–î.
	-	–ò—Å–ø–æ–ª—å–∑—É–µ—Ç –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä Catalyst –∏ Tungsten (—Å—Ç–æ–ª–±—Ü–æ–≤–∞—è –ø–∞–º—è—Ç—å, –∫–æ–¥–æ–≥–µ–Ω–µ—Ä–∞—Ü–∏—è).
	-	–£–¥–æ–±–µ–Ω –¥–ª—è SQL –∏ –∞–Ω–∞–ª–∏—Ç–∏–∫–∏.
	-	–¢—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏–∏ ‚Äî –ª–µ–Ω–∏–≤—ã–µ –æ–ø–µ—Ä–∞—Ü–∏–∏ (map, filter, select, join, groupBy) ‚Äî –≤—ã–ø–æ–ª–Ω—è—é—Ç—Å—è –ø—Ä–∏ –≤—ã–∑–æ–≤–µ –¥–µ–π—Å—Ç–≤–∏—è.
	-	–î–µ–π—Å—Ç–≤–∏—è ‚Äî –∑–∞–ø—É—Å–∫–∞—é—Ç –≤—ã—á–∏—Å–ª–µ–Ω–∏—è (count, collect, save, write).

---

3. –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –æ–∫—Ä—É–∂–µ–Ω–∏—è
	1.	–°–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç—å –≤–µ—Ä—Å–∏–π: Spark ‚Üî Hadoop/HDFS (–ø—Ä–æ–≤–µ—Ä—è–π—Ç–µ –º–∞—Ç—Ä–∏—Ü—É).
	2.	–ö–æ–Ω—Ñ–∏–≥–∏ Hadoop: core-site.xml, hdfs-site.xml ‚Üí –¥–æ–ª–∂–Ω—ã –±—ã—Ç—å –¥–æ—Å—Ç—É–ø–Ω—ã Spark.
–ß–∞—Å—Ç–∞—è –æ—à–∏–±–∫–∞: Spark –Ω–µ –≤–∏–¥–∏—Ç HDFS –∏ —Ä–∞–±–æ—Ç–∞–µ—Ç —Å –ª–æ–∫–∞–ª—å–Ω–æ–π FS.
	3.	Kerberos/–ë–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç—å: kinit, keytab, –Ω–∞—Å—Ç—Ä–æ–π–∫–∞ WebHDFS (SPNEGO).
	4.	–ü—É—Ç—å –¥–æ—Å—Ç—É–ø–∞: hdfs://<namenode>:<port>/path/... –∏–ª–∏ fs.defaultFS.
	5.	–ó–∞–ø—É—Å–∫:
	-	–†–∞–∑—Ä–∞–±–æ—Ç–∫–∞: local[*], pyspark –≤ –Ω–æ—É—Ç–±—É–∫–µ.
	-	–ü—Ä–æ–¥: spark-submit --master yarn / --master spark://....

---

4. –ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ PySpark –∫ HDFS ‚Äî –ø—Ä–∏–º–µ—Ä

```
from pyspark.sql import SparkSession
from pyspark.sql.types import StructType, StructField, StringType, IntegerType

spark = SparkSession.builder \
    .appName("hdfs-pyspark-example") \
    .getOrCreate()

schema = StructType([
    StructField("user_id", IntegerType(), True),
    StructField("event_ts", StringType(), True),
    StructField("event_type", StringType(), True)
])

df = spark.read.schema(schema) \
    .option("header", "true") \
    .csv("hdfs://namenode:9000/data/events.csv")

from pyspark.sql import functions as F
df = df.withColumn("event_ts", F.to_timestamp("event_ts", "yyyy-MM-dd HH:mm:ss"))
df.show(5)

# –ó–∞–ø–∏—Å—å –≤ Parquet —Å –ø–∞—Ä—Ç–∏—Ü–∏–æ–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ–º
df = df.withColumn("year", F.year("event_ts")) \
       .withColumn("month", F.month("event_ts"))

df.write.mode("overwrite").partitionBy("year", "month") \
    .parquet("hdfs://namenode:9000/data/parquet/events")
```


---

5. RDD ‚Äî —á—Ç–æ —ç—Ç–æ, –ø—Ä–∏–º–µ—Ä WordCount

–ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –¥–ª—è –Ω–∏–∑–∫–æ—É—Ä–æ–≤–Ω–µ–≤–æ–≥–æ –∫–æ–Ω—Ç—Ä–æ–ª—è –∏–ª–∏ —Å–ø–µ—Ü–∏—Ñ–∏—á–µ—Å–∫–∏—Ö –æ–ø–µ—Ä–∞—Ü–∏–π.
```
from pyspark.sql import SparkSession

spark = SparkSession.builder.appName("rdd-wordcount").getOrCreate()
sc = spark.sparkContext

rdd = sc.textFile("hdfs://namenode:9000/data/input.txt")
words = rdd.flatMap(lambda line: line.split())
pairs = words.map(lambda w: (w.lower(), 1))
counts = pairs.reduceByKey(lambda a, b: a + b)

# –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç –≤ HDFS
counts.map(lambda kv: f"{kv[0]}\t{kv[1]}").saveAsTextFile(
    "hdfs://namenode:9000/data/output_wordcount"
)
```
üìå –£ RDD –µ—Å—Ç—å lineage ‚Äî —Ü–µ–ø–æ—á–∫–∞ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–π, –∫–æ—Ç–æ—Ä–∞—è –ø–æ–º–æ–≥–∞–µ—Ç –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–∏—Ç—å—Å—è –ø—Ä–∏ —Å–±–æ–µ.

RDD ‚Äî —ç—Ç–æ —É—Å—Ç–æ–π—á–∏–≤—ã–π —Ä–∞—Å–ø—Ä–µ–¥–µ–ª—ë–Ω–Ω—ã–π –Ω–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö (Resilient Distributed Dataset).
–≠—Ç–æ –±–∞–∑–æ–≤—ã–π –æ–±—ä–µ–∫—Ç –≤ Apache Spark, –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è—é—â–∏–π —Å–æ–±–æ–π —Ä–∞—Å–ø—Ä–µ–¥–µ–ª—ë–Ω–Ω—É—é –∫–æ–ª–ª–µ–∫—Ü–∏—é —ç–ª–µ–º–µ–Ω—Ç–æ–≤, —Ä–∞–∑–¥–µ–ª—ë–Ω–Ω—ã—Ö –Ω–∞ –ø–∞—Ä—Ç–∏—Ü–∏–∏ –∏ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º—ã—Ö –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ –Ω–∞ –∫–ª–∞—Å—Ç–µ—Ä–µ.

–ö–ª—é—á–µ–≤—ã–µ —Å–≤–æ–π—Å—Ç–≤–∞ RDD:
-	Resilient (—É—Å—Ç–æ–π—á–∏–≤—ã–π): –ø—Ä–∏ —Å–±–æ–µ —É–∑–ª–∞ –¥–∞–Ω–Ω—ã–µ –º–æ–∂–Ω–æ –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–∏—Ç—å –∏–∑ lineage (—Ü–µ–ø–æ—á–∫–∏ –æ–ø–µ—Ä–∞—Ü–∏–π).
-	Distributed (—Ä–∞—Å–ø—Ä–µ–¥–µ–ª—ë–Ω–Ω—ã–π): –¥–∞–Ω–Ω—ã–µ —Ö—Ä–∞–Ω—è—Ç—Å—è –∏ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞—é—Ç—Å—è –Ω–∞ —Ä–∞–∑–Ω—ã—Ö —É–∑–ª–∞—Ö –∫–ª–∞—Å—Ç–µ—Ä–∞.
-	Dataset (–Ω–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö): –∫–æ–ª–ª–µ–∫—Ü–∏—è —ç–ª–µ–º–µ–Ω—Ç–æ–≤, –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –¥–ª—è —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏–π –∏ –¥–µ–π—Å—Ç–≤–∏–π.

‚∏ª

–ö–∞–∫ —É—Å—Ç—Ä–æ–µ–Ω RDD?
1.	–ü–∞—Ä—Ç–∏—Ü–∏–æ–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ
	-	–î–∞–Ω–Ω—ã–µ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –¥–µ–ª—è—Ç—Å—è –Ω–∞ –ø–∞—Ä—Ç–∏—Ü–∏–∏.
	-	–ö–∞–∂–¥–∞—è –ø–∞—Ä—Ç–∏—Ü–∏—è –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç—Å—è –æ—Ç–¥–µ–ª—å–Ω—ã–º executor-–æ–º.
2.	Lineage (—Ä–æ–¥–æ—Å–ª–æ–≤–Ω–∞—è)
	-	Spark –Ω–µ —Ö—Ä–∞–Ω–∏—Ç –ø—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã, –∞ —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç –ø–ª–∞–Ω (–≥—Ä–∞—Ñ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–π).
	-	–ü—Ä–∏ —Å–±–æ–µ –ø–∞—Ä—Ç–∏—Ü–∏–∏ Spark –ø–µ—Ä–µ—Å—á–∏—Ç–∞–µ—Ç –µ—ë –∑–∞–Ω–æ–≤–æ, —Å–ª–µ–¥—É—è —Ü–µ–ø–æ—á–∫–µ –æ–ø–µ—Ä–∞—Ü–∏–π.
3.	–õ–µ–Ω–∏–≤—ã–µ –≤—ã—á–∏—Å–ª–µ–Ω–∏—è (Lazy evaluation)
	-	–¢—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏–∏ (map, filter, flatMap, reduceByKey) –Ω–µ –≤—ã–ø–æ–ª–Ω—è—é—Ç—Å—è —Å—Ä–∞–∑—É.
	-	Spark —Å—Ç—Ä–æ–∏—Ç DAG (Directed Acyclic Graph) –æ–ø–µ—Ä–∞—Ü–∏–π.
	-	–í—ã—á–∏—Å–ª–µ–Ω–∏—è –∑–∞–ø—É—Å–∫–∞—é—Ç—Å—è —Ç–æ–ª—å–∫–æ –ø—Ä–∏ –≤—ã–∑–æ–≤–µ –¥–µ–π—Å—Ç–≤–∏—è (collect, count, saveAsTextFile).

‚∏ª

–¢–∏–ø—ã –æ–ø–µ—Ä–∞—Ü–∏–π —Å RDD
-	–¢—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏–∏ (–≤–æ–∑–≤—Ä–∞—â–∞—é—Ç –Ω–æ–≤—ã–π RDD, –ª–µ–Ω–∏–≤—ã–µ):
-	map(), filter(), flatMap(), reduceByKey(), groupByKey()
-	–î–µ–ª—è—Ç—Å—è –Ω–∞:
	-	narrow (—É–∑–∫–∏–µ) ‚Äî –Ω–µ —Ç—Ä–µ–±—É—é—Ç shuffle (–Ω–∞–ø—Ä–∏–º–µ—Ä, map, filter).
	-	wide (—à–∏—Ä–æ–∫–∏–µ) ‚Äî —Ç—Ä–µ–±—É—é—Ç shuffle (–Ω–∞–ø—Ä–∏–º–µ—Ä, groupByKey, reduceByKey).
	-	–î–µ–π—Å—Ç–≤–∏—è (–∑–∞–ø—É—Å–∫–∞—é—Ç –≤—ã—á–∏—Å–ª–µ–Ω–∏—è, –≤–æ–∑–≤—Ä–∞—â–∞—é—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç –∏–ª–∏ –ø–∏—à—É—Ç –≤ HDFS):
	-	collect(), count(), saveAsTextFile(), reduce(), first().

```mermaid
flowchart TD
    A[HDFS: input.txt]
    B[sc.textFile()]
    C[flatMap(line -> words)]
    D[map(word -> (word,1))]
    E[reduceByKey(a+b)]
    F[saveAsTextFile(output)]

    A --> B
    B --> C
    C --> D
    D --> E
    E --> F
```

‚∏ª

–ö–æ–≥–¥–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å RDD?

-	–ö–æ–≥–¥–∞ –Ω—É–∂–Ω–∞ –Ω–∏–∑–∫–æ—É—Ä–æ–≤–Ω–µ–≤–∞—è —Ä–∞–±–æ—Ç–∞ —Å –¥–∞–Ω–Ω—ã–º–∏.
-	–ï—Å–ª–∏ –æ–ø–µ—Ä–∞—Ü–∏–∏ –Ω–µ —É–∫–ª–∞–¥—ã–≤–∞—é—Ç—Å—è –≤ DataFrame API –∏–ª–∏ SQL.
-	–ü—Ä–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–∏ –Ω–µ—Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã—Ö —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏–π, –∫–æ—Ç–æ—Ä—ã–µ –Ω–µ–≤–æ–∑–º–æ–∂–Ω–æ –≤—ã—Ä–∞–∑–∏—Ç—å SQL-–∑–∞–ø—Ä–æ—Å–æ–º.
-	–î–ª—è fine-grained –∫–æ–Ω—Ç—Ä–æ–ª—è –∑–∞ –ø–∞—Ä—Ç–∏—Ü–∏–æ–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ–º –∏ —É–ø—Ä–∞–≤–ª–µ–Ω–∏–µ–º –¥–∞–Ω–Ω—ã–º–∏.

üìå –û–¥–Ω–∞–∫–æ: –¥–ª—è ETL –∏ –∞–Ω–∞–ª–∏—Ç–∏–∫–∏ –≤ Python –ª—É—á—à–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å DataFrame API ‚Äî –æ–Ω –±—ã—Å—Ç—Ä–µ–µ –±–ª–∞–≥–æ–¥–∞—Ä—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è–º Catalyst –∏ Tungsten.


---

6. DataFrame (–¢–î) ‚Äî –ø—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞ –∏ –ø—Ä–∏–º–µ—Ä
	-	–ò—Å–ø–æ–ª—å–∑—É–µ—Ç –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ Catalyst + Tungsten.
	-	–ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç SQL, —É–¥–æ–±–µ–Ω –¥–ª—è ETL.
	-	–§–æ—Ä–º–∞—Ç—ã: Parquet/ORC (—Ä–µ–∫–æ–º–µ–Ω–¥—É—é—Ç—Å—è), Avro, CSV, JSON.
```
from pyspark.sql import SparkSession
from pyspark.sql.functions import col, count

spark = SparkSession.builder.appName("df-example").getOrCreate()

df = spark.read.parquet("hdfs://namenode:9000/data/parquet/events")

res = df.filter(col("event_type") == "purchase") \
        .groupBy("user_id") \
        .agg(count("*").alias("purchases"))

res.write.mode("overwrite").parquet(
    "hdfs://namenode:9000/data/parquet/purchase_per_user"
)
```


---

7. –õ—É—á—à–∏–µ –ø—Ä–∞–∫—Ç–∏–∫–∏ —Ä–∞–±–æ—Ç—ã —Å –¥–∞–Ω–Ω—ã–º–∏

a) –§–æ—Ä–º–∞—Ç—ã
-	Parquet / ORC ‚Äî –¥–ª—è –∞–Ω–∞–ª–∏—Ç–∏–∫–∏.
-	JSON/Avro ‚Äî –¥–ª—è –ª–æ–≥–æ–≤ –∏ —Å—Ç—Ä–∏–º–∏–Ω–≥–∞.

b) –ü–∞—Ä—Ç–∏—Ü–∏–æ–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ
-	–ü–∞—Ä—Ç–∏—Ü–∏–æ–Ω–∏—Ä—É–π—Ç–µ –ø–æ –∫–æ–ª–æ–Ω–∫–∞–º —Å —á–∞—Å—Ç—ã–º–∏ —Ñ–∏–ª—å—Ç—Ä–∞–º–∏ (–Ω–∞–ø—Ä–∏–º–µ—Ä, date).
-	–ò–∑–±–µ–≥–∞–π—Ç–µ small files problem ‚Äî –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ repartition –∏–ª–∏ coalesce.

c) –ú–∏–Ω–∏–º–∏–∑–∞—Ü–∏—è shuffle
-	–ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ broadcast join –¥–ª—è –º–∞–ª–µ–Ω—å–∫–∏—Ö —Ç–∞–±–ª–∏—Ü.
-	–ù–∞—Å—Ç—Ä–∞–∏–≤–∞–π—Ç–µ spark.sql.shuffle.partitions.

d) –ö—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ
-	df.cache() / df.persist() ‚Äî —Ç–æ–ª—å–∫–æ –¥–ª—è —á–∞—Å—Ç–æ –∏—Å–ø–æ–ª—å–∑—É–µ–º—ã—Ö.
-	–î–ª—è –¥–ª–∏–Ω–Ω–æ–π lineage –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ checkpoint.

e) PySpark-–æ—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–∏
-	–û–±—ã—á–Ω—ã–µ Python UDF –º–µ–¥–ª–µ–Ω–Ω—ã–µ ‚Üí –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ pandas UDF + Arrow.
-	–î–ª—è –≤–Ω–µ—à–Ω–∏—Ö —Å–æ–µ–¥–∏–Ω–µ–Ω–∏–π –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ mapPartitions.

f) –°–∂–∞—Ç–∏–µ
-	–û–±—ã—á–Ω–æ Parquet —Å–æ Snappy.

g) –°—Ö–µ–º–∞
-	–ó–∞–¥–∞–≤–∞–π—Ç–µ —è–≤–Ω–æ (.schema(...)), –æ—Å–æ–±–µ–Ω–Ω–æ –¥–ª—è CSV/JSON.

h) –†–µ—Å—É—Ä—Å—ã
-	–¢—é–Ω—å—Ç–µ --executor-memory, --executor-cores.

---

8. –¢–æ–Ω–∫–æ—Å—Ç–∏ Spark ‚Üî HDFS
	-	–ü—Ä–æ–≤–µ—Ä—è–π—Ç–µ –Ω–∞–ª–∏—á–∏–µ Hadoop jars –∏ $HADOOP_CONF_DIR.
	-	–î–ª—è secure-–∫–ª–∞—Å—Ç–µ—Ä–∞: kinit, keytab, SPNEGO –¥–ª—è WebHDFS.
	-	–ú–∞–ª–µ–Ω—å–∫–∏–µ lookup-—Ç–∞–±–ª–∏—Ü—ã ‚Üí broadcast, –∞ –Ω–µ sc.textFile.

---

9. –ü—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–µ –ø—Ä–∏–º–µ—Ä—ã

9.1 Broadcast join
```
from pyspark.sql.functions import broadcast

big = spark.read.parquet("hdfs://namenode:9000/big_table")
small = spark.read.parquet("hdfs://namenode:9000/small_ref")

joined = big.join(broadcast(small), on="key", how="left")
```
9.2 mapPartitions
```
def process_partition(iter_rows):
    conn = open_db_connection()
    for row in iter_rows:
        yield process_row_with_conn(row, conn)
    conn.close()

rdd = sc.textFile("hdfs://...").mapPartitions(process_partition)
```
9.3 Pandas UDF
```
import pandas as pd
from pyspark.sql.functions import pandas_udf
from pyspark.sql.types import DoubleType

@pandas_udf(DoubleType())
def my_vec_udf(s: pd.Series) -> pd.Series:
    return s * 2.0

df = df.withColumn("x2", my_vec_udf(df["x"]))
spark.conf.set("spark.sql.execution.arrow.pyspark.enabled", "true")
```


---

10. –°—Ç—Ä–∏–º–∏–Ω–≥: –∑–∞–ø–∏—Å—å –≤ HDFS

Structured Streaming –º–æ–∂–µ—Ç –ø–∏—Å–∞—Ç—å –≤ HDFS (–Ω–∞–ø—Ä–∏–º–µ—Ä, –≤ Parquet).
‚ö†Ô∏è –ü—Ä–æ–±–ª–µ–º–∞: —Å–æ–∑–¥–∞—ë—Ç –º–Ω–æ–≥–æ –º–∞–ª–µ–Ω—å–∫–∏—Ö —Ñ–∞–π–ª–æ–≤ ‚Üí –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ –ø–∞—Ä—Ç–∏—Ü–∏–æ–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ –∏ compact.

---

11. –û—Ç–ª–∞–¥–∫–∞ –∏ –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥
	-	Spark UI (driver:4040) ‚Äî DAG, shuffle, —Å—Ç–∞–¥–∏–∏.
	-	History Server ‚Äî –∞–Ω–∞–ª–∏–∑ –≤—ã–ø–æ–ª–Ω–µ–Ω–Ω—ã—Ö –∑–∞–¥–∞—á.
	-	–õ–æ–≥–∏ executors, GC, OOM.

---

12. –û–±—â–∏–µ –æ—à–∏–±–∫–∏
	-	Permission denied ‚Üí Kerberos / –ø—Ä–∞–≤–∞.
	-	–ú–Ω–æ–≥–æ –º–∞–ª–µ–Ω—å–∫–∏—Ö —Ñ–∞–π–ª–æ–≤ ‚Üí repartition.
	-	–ú–µ–¥–ª–µ–Ω–Ω—ã–π join ‚Üí broadcast / salting / –Ω–∞—Å—Ç—Ä–æ–π–∫–∞ shuffle.
	-	–ú–µ–¥–ª–µ–Ω–Ω—ã–µ UDF ‚Üí –∑–∞–º–µ–Ω–∏—Ç—å –Ω–∞ SQL-—Ñ—É–Ω–∫—Ü–∏–∏ –∏–ª–∏ pandas UDF.
	-	–ù–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω–∞—è —Å—Ö–µ–º–∞ ‚Üí –∑–∞–¥–∞–≤–∞—Ç—å —è–≤–Ω–æ.

---

13. –ü—Ä–∏–º–µ—Ä ETL (CSV ‚Üí Parquet)
```
from pyspark.sql import SparkSession
from pyspark.sql.functions import col, to_timestamp, year, month

spark = SparkSession.builder.appName("etl-example").getOrCreate()

schema = "user_id INT, event_ts STRING, event_type STRING, amount DOUBLE"

df = spark.read.schema(schema).option("header","true") \
         .csv("hdfs://namenode:9000/raw/events/*.csv")

clean = (df.filter(col("user_id").isNotNull())
           .withColumn("event_ts", to_timestamp(col("event_ts"), "yyyy-MM-dd HH:mm:ss"))
           .withColumn("year", year("event_ts"))
           .withColumn("month", month("event_ts")))

clean.cache()

agg = clean.filter(col("event_type") == "purchase") \
           .groupBy("year","month","user_id") \
           .agg({"amount":"sum"}) \
           .withColumnRenamed("sum(amount)","total_amount")

spark.conf.set("spark.sql.parquet.compression.codec", "snappy")

agg.write.mode("overwrite").partitionBy("year","month") \
       .parquet("hdfs://namenode:9000/warehouse/purchases")

```

---