# –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ HDFS –∏ Apache Spark (PySpark)


1. –ö–æ—Ä–æ—Ç–∫–æ: —á—Ç–æ —Ç–∞–∫–æ–µ HDFS –∏ Spark –∏ –ø–æ—á–µ–º—É –∏—Ö –≤–º–µ—Å—Ç–µ –∏—Å–ø–æ–ª—å–∑—É—é—Ç
	‚Ä¢	HDFS (Hadoop Distributed File System) ‚Äî —Ä–∞—Å–ø—Ä–µ–¥–µ–ª—ë–Ω–Ω–∞—è —Ñ–∞–π–ª–æ–≤–∞—è —Å–∏—Å—Ç–µ–º–∞ –¥–ª—è –±–æ–ª—å—à–∏—Ö –æ–±—ä—ë–º–æ–≤ –¥–∞–Ω–Ω—ã—Ö: —Ö—Ä–∞–Ω–∏—Ç —Ñ–∞–π–ª—ã, —Ä–∞–∑–±–∏–≤–∞–µ—Ç –∏—Ö –Ω–∞ –±–ª–æ–∫–∏ –∏ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª—è–µ—Ç –ø–æ DataNode. –ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ —Ö—Ä–∞–Ω–∏—Ç NameNode.
–î–∞—ë—Ç –Ω–∞–¥—ë–∂–Ω–æ–µ –∏ –º–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º–æ–µ —Ö—Ä–∞–Ω–∏–ª–∏—â–µ –¥–ª—è –∞–Ω–∞–ª–∏—Ç–∏–∫–∏.
(–¥–ª—è –¥–µ—Ç–∞–ª–µ–π: WebHDFS / –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è —Å–º. –æ—Ñ–∏—Ü–∏–∞–ª—å–Ω—É—é –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—é).
	‚Ä¢	Apache Spark ‚Äî –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω–∞—è –ø–ª–∞—Ç—Ñ–æ—Ä–º–∞ –¥–ª—è —Ä–∞—Å–ø—Ä–µ–¥–µ–ª—ë–Ω–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏ –¥–∞–Ω–Ω—ã—Ö (–ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã–µ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏–∏, SQL, —Å—Ç—Ä–∏–º–∏–Ω–≥, ML).
Spark –º–æ–∂–µ—Ç —á–∏—Ç–∞—Ç—å/–ø–∏—Å–∞—Ç—å –¥–∞–Ω–Ω—ã–µ –ø—Ä—è–º–æ –∏–∑ HDFS –∏ –≤—ã–ø–æ–ª–Ω—è—Ç—å –≤—ã—á–∏—Å–ª–µ–Ω–∏—è –Ω–∞–¥ –Ω–∏–º–∏.
–ü—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–µ—Å–∫–æ–ª—å–∫–æ API:
	‚Ä¢	RDD (–Ω–∏–∑–∫–æ—É—Ä–æ–≤–Ω–µ–≤–æ–µ)
	‚Ä¢	DataFrame / SQL (—Ç–∞–±–ª–∏—á–Ω–æ–µ, –æ–ø—Ç–∏–º–∏–∑–∏—Ä—É–µ–º–æ–µ)
	‚Ä¢	Dataset (–≤ Scala/Java)

üìå –ü–æ—á–µ–º—É –≤–º–µ—Å—Ç–µ?
HDFS ‚Äî –º–µ—Å—Ç–æ —Ö—Ä–∞–Ω–µ–Ω–∏—è, Spark ‚Äî –¥–≤–∏–∂–æ–∫ –æ–±—Ä–∞–±–æ—Ç–∫–∏.
HDFS –≥–∞—Ä–∞–Ω—Ç–∏—Ä—É–µ—Ç —Ä–µ–ø–ª–∏–∫–∞—Ü–∏—é –∏ –æ—Ç–∫–∞–∑–æ—É—Å—Ç–æ–π—á–∏–≤–æ—Å—Ç—å, Spark ‚Äî –±—ã—Å—Ç—Ä–æ–µ in-memory/–¥–∏—Å–∫–æ–≤–æ–µ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–µ.

‚∏ª

2. –ö—Ä–∞—Ç–∫–∞—è —Ç–µ—Ä–º–∏–Ω–æ–ª–æ–≥–∏—è
	‚Ä¢	RDD (Resilient Distributed Dataset) ‚Äî —Ñ—É–Ω–¥–∞–º–µ–Ω—Ç Spark: –∏–º–º—É—Ç–∞–±–µ–ª—å–Ω–∞—è —Ä–∞—Å–ø—Ä–µ–¥–µ–ª—ë–Ω–Ω–∞—è –∫–æ–ª–ª–µ–∫—Ü–∏—è —ç–ª–µ–º–µ–Ω—Ç–æ–≤.
	‚Ä¢	–õ–µ–Ω–∏–≤—ã–µ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏–∏ –∏ –¥–µ–π—Å—Ç–≤–∏—è.
	‚Ä¢	–î–∞—ë—Ç –ø–æ–ª–Ω—ã–π –∫–æ–Ω—Ç—Ä–æ–ª—å, –Ω–æ –±–µ–∑ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–π.
	‚Ä¢	DataFrame (–¢–∞–±–ª–∏—á–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ) ‚Äî —Å—Ç—Ä—É–∫—Ç—É—Ä–∞ —Å –∏–º–µ–Ω–æ–≤–∞–Ω–Ω—ã–º–∏ –∫–æ–ª–æ–Ω–∫–∞–º–∏, –∫–∞–∫ —Ç–∞–±–ª–∏—Ü–∞ –≤ –ë–î.
	‚Ä¢	–ò—Å–ø–æ–ª—å–∑—É–µ—Ç –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä Catalyst –∏ Tungsten (—Å—Ç–æ–ª–±—Ü–æ–≤–∞—è –ø–∞–º—è—Ç—å, –∫–æ–¥–æ–≥–µ–Ω–µ—Ä–∞—Ü–∏—è).
	‚Ä¢	–£–¥–æ–±–µ–Ω –¥–ª—è SQL –∏ –∞–Ω–∞–ª–∏—Ç–∏–∫–∏.
	‚Ä¢	–¢—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏–∏ ‚Äî –ª–µ–Ω–∏–≤—ã–µ –æ–ø–µ—Ä–∞—Ü–∏–∏ (map, filter, select, join, groupBy) ‚Äî –≤—ã–ø–æ–ª–Ω—è—é—Ç—Å—è –ø—Ä–∏ –≤—ã–∑–æ–≤–µ –¥–µ–π—Å—Ç–≤–∏—è.
	‚Ä¢	–î–µ–π—Å—Ç–≤–∏—è ‚Äî –∑–∞–ø—É—Å–∫–∞—é—Ç –≤—ã—á–∏—Å–ª–µ–Ω–∏—è (count, collect, save, write).

‚∏ª

3. –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –æ–∫—Ä—É–∂–µ–Ω–∏—è
	1.	–°–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç—å –≤–µ—Ä—Å–∏–π: Spark ‚Üî Hadoop/HDFS (–ø—Ä–æ–≤–µ—Ä—è–π—Ç–µ –º–∞—Ç—Ä–∏—Ü—É).
	2.	–ö–æ–Ω—Ñ–∏–≥–∏ Hadoop: core-site.xml, hdfs-site.xml ‚Üí –¥–æ–ª–∂–Ω—ã –±—ã—Ç—å –¥–æ—Å—Ç—É–ø–Ω—ã Spark.
–ß–∞—Å—Ç–∞—è –æ—à–∏–±–∫–∞: Spark –Ω–µ –≤–∏–¥–∏—Ç HDFS –∏ —Ä–∞–±–æ—Ç–∞–µ—Ç —Å –ª–æ–∫–∞–ª—å–Ω–æ–π FS.
	3.	Kerberos/–ë–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç—å: kinit, keytab, –Ω–∞—Å—Ç—Ä–æ–π–∫–∞ WebHDFS (SPNEGO).
	4.	–ü—É—Ç—å –¥–æ—Å—Ç—É–ø–∞: hdfs://<namenode>:<port>/path/... –∏–ª–∏ fs.defaultFS.
	5.	–ó–∞–ø—É—Å–∫:
	‚Ä¢	–†–∞–∑—Ä–∞–±–æ—Ç–∫–∞: local[*], pyspark –≤ –Ω–æ—É—Ç–±—É–∫–µ.
	‚Ä¢	–ü—Ä–æ–¥: spark-submit --master yarn / --master spark://....

‚∏ª

4. –ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ PySpark –∫ HDFS ‚Äî –ø—Ä–∏–º–µ—Ä

```
from pyspark.sql import SparkSession
from pyspark.sql.types import StructType, StructField, StringType, IntegerType

spark = SparkSession.builder \
    .appName("hdfs-pyspark-example") \
    .getOrCreate()

schema = StructType([
    StructField("user_id", IntegerType(), True),
    StructField("event_ts", StringType(), True),
    StructField("event_type", StringType(), True)
])

df = spark.read.schema(schema) \
    .option("header", "true") \
    .csv("hdfs://namenode:9000/data/events.csv")

from pyspark.sql import functions as F
df = df.withColumn("event_ts", F.to_timestamp("event_ts", "yyyy-MM-dd HH:mm:ss"))
df.show(5)

# –ó–∞–ø–∏—Å—å –≤ Parquet —Å –ø–∞—Ä—Ç–∏—Ü–∏–æ–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ–º
df = df.withColumn("year", F.year("event_ts")) \
       .withColumn("month", F.month("event_ts"))

df.write.mode("overwrite").partitionBy("year", "month") \
    .parquet("hdfs://namenode:9000/data/parquet/events")
```


‚∏ª

5. RDD ‚Äî —á—Ç–æ —ç—Ç–æ, –ø—Ä–∏–º–µ—Ä WordCount

–ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –¥–ª—è –Ω–∏–∑–∫–æ—É—Ä–æ–≤–Ω–µ–≤–æ–≥–æ –∫–æ–Ω—Ç—Ä–æ–ª—è –∏–ª–∏ —Å–ø–µ—Ü–∏—Ñ–∏—á–µ—Å–∫–∏—Ö –æ–ø–µ—Ä–∞—Ü–∏–π.
```
from pyspark.sql import SparkSession

spark = SparkSession.builder.appName("rdd-wordcount").getOrCreate()
sc = spark.sparkContext

rdd = sc.textFile("hdfs://namenode:9000/data/input.txt")
words = rdd.flatMap(lambda line: line.split())
pairs = words.map(lambda w: (w.lower(), 1))
counts = pairs.reduceByKey(lambda a, b: a + b)

# –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç –≤ HDFS
counts.map(lambda kv: f"{kv[0]}\t{kv[1]}").saveAsTextFile(
    "hdfs://namenode:9000/data/output_wordcount"
)
```
üìå –£ RDD –µ—Å—Ç—å lineage ‚Äî —Ü–µ–ø–æ—á–∫–∞ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–π, –∫–æ—Ç–æ—Ä–∞—è –ø–æ–º–æ–≥–∞–µ—Ç –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–∏—Ç—å—Å—è –ø—Ä–∏ —Å–±–æ–µ.

‚∏ª

6. DataFrame (–¢–î) ‚Äî –ø—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞ –∏ –ø—Ä–∏–º–µ—Ä
	‚Ä¢	–ò—Å–ø–æ–ª—å–∑—É–µ—Ç –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ Catalyst + Tungsten.
	‚Ä¢	–ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç SQL, —É–¥–æ–±–µ–Ω –¥–ª—è ETL.
	‚Ä¢	–§–æ—Ä–º–∞—Ç—ã: Parquet/ORC (—Ä–µ–∫–æ–º–µ–Ω–¥—É—é—Ç—Å—è), Avro, CSV, JSON.
```
from pyspark.sql import SparkSession
from pyspark.sql.functions import col, count

spark = SparkSession.builder.appName("df-example").getOrCreate()

df = spark.read.parquet("hdfs://namenode:9000/data/parquet/events")

res = df.filter(col("event_type") == "purchase") \
        .groupBy("user_id") \
        .agg(count("*").alias("purchases"))

res.write.mode("overwrite").parquet(
    "hdfs://namenode:9000/data/parquet/purchase_per_user"
)
```


‚∏ª

7. –õ—É—á—à–∏–µ –ø—Ä–∞–∫—Ç–∏–∫–∏ —Ä–∞–±–æ—Ç—ã —Å –¥–∞–Ω–Ω—ã–º–∏

a) –§–æ—Ä–º–∞—Ç—ã
	‚Ä¢	Parquet / ORC ‚Äî –¥–ª—è –∞–Ω–∞–ª–∏—Ç–∏–∫–∏.
	‚Ä¢	JSON/Avro ‚Äî –¥–ª—è –ª–æ–≥–æ–≤ –∏ —Å—Ç—Ä–∏–º–∏–Ω–≥–∞.

b) –ü–∞—Ä—Ç–∏—Ü–∏–æ–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ
	‚Ä¢	–ü–∞—Ä—Ç–∏—Ü–∏–æ–Ω–∏—Ä—É–π—Ç–µ –ø–æ –∫–æ–ª–æ–Ω–∫–∞–º —Å —á–∞—Å—Ç—ã–º–∏ —Ñ–∏–ª—å—Ç—Ä–∞–º–∏ (–Ω–∞–ø—Ä–∏–º–µ—Ä, date).
	‚Ä¢	–ò–∑–±–µ–≥–∞–π—Ç–µ small files problem ‚Äî –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ repartition –∏–ª–∏ coalesce.

c) –ú–∏–Ω–∏–º–∏–∑–∞—Ü–∏—è shuffle
	‚Ä¢	–ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ broadcast join –¥–ª—è –º–∞–ª–µ–Ω—å–∫–∏—Ö —Ç–∞–±–ª–∏—Ü.
	‚Ä¢	–ù–∞—Å—Ç—Ä–∞–∏–≤–∞–π—Ç–µ spark.sql.shuffle.partitions.

d) –ö—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ
	‚Ä¢	df.cache() / df.persist() ‚Äî —Ç–æ–ª—å–∫–æ –¥–ª—è —á–∞—Å—Ç–æ –∏—Å–ø–æ–ª—å–∑—É–µ–º—ã—Ö.
	‚Ä¢	–î–ª—è –¥–ª–∏–Ω–Ω–æ–π lineage –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ checkpoint.

e) PySpark-–æ—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–∏
	‚Ä¢	–û–±—ã—á–Ω—ã–µ Python UDF –º–µ–¥–ª–µ–Ω–Ω—ã–µ ‚Üí –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ pandas UDF + Arrow.
	‚Ä¢	–î–ª—è –≤–Ω–µ—à–Ω–∏—Ö —Å–æ–µ–¥–∏–Ω–µ–Ω–∏–π –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ mapPartitions.

f) –°–∂–∞—Ç–∏–µ
	‚Ä¢	–û–±—ã—á–Ω–æ Parquet —Å–æ Snappy.

g) –°—Ö–µ–º–∞
	‚Ä¢	–ó–∞–¥–∞–≤–∞–π—Ç–µ —è–≤–Ω–æ (.schema(...)), –æ—Å–æ–±–µ–Ω–Ω–æ –¥–ª—è CSV/JSON.

h) –†–µ—Å—É—Ä—Å—ã
	‚Ä¢	–¢—é–Ω—å—Ç–µ --executor-memory, --executor-cores.

‚∏ª

8. –¢–æ–Ω–∫–æ—Å—Ç–∏ Spark ‚Üî HDFS
	‚Ä¢	–ü—Ä–æ–≤–µ—Ä—è–π—Ç–µ –Ω–∞–ª–∏—á–∏–µ Hadoop jars –∏ $HADOOP_CONF_DIR.
	‚Ä¢	–î–ª—è secure-–∫–ª–∞—Å—Ç–µ—Ä–∞: kinit, keytab, SPNEGO –¥–ª—è WebHDFS.
	‚Ä¢	–ú–∞–ª–µ–Ω—å–∫–∏–µ lookup-—Ç–∞–±–ª–∏—Ü—ã ‚Üí broadcast, –∞ –Ω–µ sc.textFile.

‚∏ª

9. –ü—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–µ –ø—Ä–∏–º–µ—Ä—ã

9.1 Broadcast join
```
from pyspark.sql.functions import broadcast

big = spark.read.parquet("hdfs://namenode:9000/big_table")
small = spark.read.parquet("hdfs://namenode:9000/small_ref")

joined = big.join(broadcast(small), on="key", how="left")
```
9.2 mapPartitions
```
def process_partition(iter_rows):
    conn = open_db_connection()
    for row in iter_rows:
        yield process_row_with_conn(row, conn)
    conn.close()

rdd = sc.textFile("hdfs://...").mapPartitions(process_partition)
```
9.3 Pandas UDF
```
import pandas as pd
from pyspark.sql.functions import pandas_udf
from pyspark.sql.types import DoubleType

@pandas_udf(DoubleType())
def my_vec_udf(s: pd.Series) -> pd.Series:
    return s * 2.0

df = df.withColumn("x2", my_vec_udf(df["x"]))
spark.conf.set("spark.sql.execution.arrow.pyspark.enabled", "true")
```


‚∏ª

10. –°—Ç—Ä–∏–º–∏–Ω–≥: –∑–∞–ø–∏—Å—å –≤ HDFS

Structured Streaming –º–æ–∂–µ—Ç –ø–∏—Å–∞—Ç—å –≤ HDFS (–Ω–∞–ø—Ä–∏–º–µ—Ä, –≤ Parquet).
‚ö†Ô∏è –ü—Ä–æ–±–ª–µ–º–∞: —Å–æ–∑–¥–∞—ë—Ç –º–Ω–æ–≥–æ –º–∞–ª–µ–Ω—å–∫–∏—Ö —Ñ–∞–π–ª–æ–≤ ‚Üí –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ –ø–∞—Ä—Ç–∏—Ü–∏–æ–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ –∏ compact.

‚∏ª

11. –û—Ç–ª–∞–¥–∫–∞ –∏ –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥
	‚Ä¢	Spark UI (driver:4040) ‚Äî DAG, shuffle, —Å—Ç–∞–¥–∏–∏.
	‚Ä¢	History Server ‚Äî –∞–Ω–∞–ª–∏–∑ –≤—ã–ø–æ–ª–Ω–µ–Ω–Ω—ã—Ö –∑–∞–¥–∞—á.
	‚Ä¢	–õ–æ–≥–∏ executors, GC, OOM.

‚∏ª

12. –û–±—â–∏–µ –æ—à–∏–±–∫–∏
	‚Ä¢	Permission denied ‚Üí Kerberos / –ø—Ä–∞–≤–∞.
	‚Ä¢	–ú–Ω–æ–≥–æ –º–∞–ª–µ–Ω—å–∫–∏—Ö —Ñ–∞–π–ª–æ–≤ ‚Üí repartition.
	‚Ä¢	–ú–µ–¥–ª–µ–Ω–Ω—ã–π join ‚Üí broadcast / salting / –Ω–∞—Å—Ç—Ä–æ–π–∫–∞ shuffle.
	‚Ä¢	–ú–µ–¥–ª–µ–Ω–Ω—ã–µ UDF ‚Üí –∑–∞–º–µ–Ω–∏—Ç—å –Ω–∞ SQL-—Ñ—É–Ω–∫—Ü–∏–∏ –∏–ª–∏ pandas UDF.
	‚Ä¢	–ù–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω–∞—è —Å—Ö–µ–º–∞ ‚Üí –∑–∞–¥–∞–≤–∞—Ç—å —è–≤–Ω–æ.

‚∏ª

13. –ü—Ä–∏–º–µ—Ä ETL (CSV ‚Üí Parquet)
```
from pyspark.sql import SparkSession
from pyspark.sql.functions import col, to_timestamp, year, month

spark = SparkSession.builder.appName("etl-example").getOrCreate()

schema = "user_id INT, event_ts STRING, event_type STRING, amount DOUBLE"

df = spark.read.schema(schema).option("header","true") \
         .csv("hdfs://namenode:9000/raw/events/*.csv")

clean = (df.filter(col("user_id").isNotNull())
           .withColumn("event_ts", to_timestamp(col("event_ts"), "yyyy-MM-dd HH:mm:ss"))
           .withColumn("year", year("event_ts"))
           .withColumn("month", month("event_ts")))

clean.cache()

agg = clean.filter(col("event_type") == "purchase") \
           .groupBy("year","month","user_id") \
           .agg({"amount":"sum"}) \
           .withColumnRenamed("sum(amount)","total_amount")

spark.conf.set("spark.sql.parquet.compression.codec", "snappy")

agg.write.mode("overwrite").partitionBy("year","month") \
       .parquet("hdfs://namenode:9000/warehouse/purchases")

```

‚∏ª

14. –®–ø–∞—Ä–≥–∞–ª–∫–∞
	‚Ä¢	–í Python –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ DataFrame API.
	‚Ä¢	–•—Ä–∞–Ω–∏—Ç–µ –¥–∞–Ω–Ω—ã–µ –≤ Parquet/ORC —Å –ø–∞—Ä—Ç–∏—Ü–∏–æ–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ–º.
	‚Ä¢	–ö–æ–Ω—Ç—Ä–æ–ª–∏—Ä—É–π—Ç–µ —á–∏—Å–ª–æ —Ñ–∞–π–ª–æ–≤ (repartition).
	‚Ä¢	–ú–∏–Ω–∏–º–∏–∑–∏—Ä—É–π—Ç–µ shuffle (broadcast, repartition).
	‚Ä¢	–ù–∞—Å—Ç—Ä–æ–π—Ç–µ –¥–æ—Å—Ç—É–ø –∫ HDFS (–∫–æ–Ω—Ñ–∏–≥–∏, Kerberos).

‚∏ª

15. –†–µ—Å—É—Ä—Å—ã
	‚Ä¢	Spark SQL, DataFrames and Datasets Guide
	‚Ä¢	RDD Programming Guide
	‚Ä¢	Databricks: —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ RDD vs DataFrame vs Dataset
	‚Ä¢	WebHDFS –∏ Kerberos ‚Äî –æ—Ñ–∏—Ü–∏–∞–ª—å–Ω–∞—è –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è Hadoop

‚∏ª

–•–æ—á–µ—à—å, —è —Å–¥–µ–ª–∞—é –µ—â—ë —Å—Ö–µ–º—É-–∏–ª–ª—é—Å—Ç—Ä–∞—Ü–∏—é (–Ω–∞–ø—Ä–∏–º–µ—Ä, Spark ‚Üî HDFS —Å DataFrame/RDD) –≤ –≤–∏–¥–µ –∫–∞—Ä—Ç–∏–Ω–∫–∏? –≠—Ç–æ –º–æ–∂–Ω–æ –≤—Å—Ç–∞–≤–∏—Ç—å –≤ —Ç–≤–æ–π .md –∫–∞–∫ –¥–∏–∞–≥—Ä–∞–º–º—É.