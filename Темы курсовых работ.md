### Примерные темы курсовых работ

Курсовая работа, направленная на построение ETL-процессов обязательно должна содержать:
 - Модуль для распределенной обработки данных
 - Модуль для стриминга данных
 - Модуль визуализации данных

Список примерных тем:
1)	Разработка и внедрение конвейера обработки больших данных на основе Apache Hadoop и Spark.
2)	Исследование производительности различных систем распределённого хранения данных (HDFS, Ceph, MinIO).
3)	Проектирование и реализация ETL-процесса для интеграции разнородных источников данных с использованием Apache NiFi.
4)	Построение системы потоковой обработки данных на базе Apache Kafka и Spark Streaming.
5)	Анализ логов веб-сервера в реальном времени с использованием Elasticsearch, Logstash и Kibana (ELK-стек).
6)	Разработка системы мониторинга инфраструктуры Big Data-кластера с использованием Prometheus и Grafana.
7)	Сравнительный анализ эффективности хранения и выборки данных в реляционных и NoSQL-системах (PostgreSQL vs MongoDB vs Cassandra).
8)	Разработка аналитического дашборда для визуализации показателей большого объёма данных (Power BI / Kibana / Superset).
9)	Оптимизация производительности Apache Spark-приложений при обработке больших наборов данных.
10)	Разработка прототипа корпоративного хранилища данных (Data Warehouse) с использованием Apache Hive.
11)	Обеспечение безопасности и разграничения доступа в системах обработки больших данных.
12)	Управление качеством данных и реализация принципов Data Governance в распределённой среде.
13)	Использование облачных решений для хранения и анализа больших данных (AWS EMR, Google BigQuery, Azure Synapse).
14)	Анализ потоков IoT-данных с помощью Apache Kafka и Elasticsearch.
15)	Разработка системы прогнозирования нагрузки на основе временных рядов с применением Spark и ClickHouse.
16)	Создание конвейера обработки данных социальных сетей: сбор, очистка, анализ, визуализация.
17)	Исследование масштабируемости и отказоустойчивости Hadoop-кластера при увеличении объёма данных.
18)	Разработка системы сбора и анализа телеметрии производственного оборудования (Industrial IoT Data Pipeline).
19)	Сравнение производительности форматов хранения данных (CSV, Parquet, ORC, Avro) при работе в Spark.
20)	Разработка и тестирование системы резервного копирования и восстановления данных в распределённой среде.
